{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Lab: Reproducibility & Environments\n",
    "\n",
    "**CS 203: Software Tools and Techniques for AI**\n",
    "\n",
    "In this lab, you'll learn to:\n",
    "1. Set random seeds for reproducibility\n",
    "2. Create and manage virtual environments\n",
    "3. Generate requirements.txt\n",
    "4. Use config files\n",
    "5. Create a proper project structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Reproducibility Problem\n",
    "\n",
    "Let's see why random seeds matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "X = np.random.randn(200, 5)\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "\n",
    "# Train WITHOUT random seed\n",
    "print(\"Training WITHOUT random seed (run multiple times):\")\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    model = RandomForestClassifier(n_estimators=10)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"  Run {i+1}: Accuracy = {model.score(X_test, y_test):.3f}\")\n",
    "\n",
    "print(\"\\nNotice how the accuracy varies each time!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train WITH random seed\n",
    "print(\"Training WITH random seed (run multiple times):\")\n",
    "for i in range(3):\n",
    "    np.random.seed(42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"  Run {i+1}: Accuracy = {model.score(X_test, y_test):.3f}\")\n",
    "\n",
    "print(\"\\nNow the accuracy is exactly the same every time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: A Complete Seed Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set all random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch (if available)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # TensorFlow (if available)\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.random.set_seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "# Call at the start of every script\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that it works\n",
    "set_seed(42)\n",
    "print(\"First sequence:\", [random.randint(1, 100) for _ in range(5)])\n",
    "\n",
    "set_seed(42)\n",
    "print(\"Same sequence:\", [random.randint(1, 100) for _ in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Virtual Environments (Terminal Commands)\n",
    "\n",
    "Run these commands in your terminal, not in this notebook.\n",
    "\n",
    "### Creating a Virtual Environment\n",
    "\n",
    "```bash\n",
    "# Navigate to your project directory\n",
    "cd my_project\n",
    "\n",
    "# Create virtual environment\n",
    "python -m venv venv\n",
    "\n",
    "# Activate it (Mac/Linux)\n",
    "source venv/bin/activate\n",
    "\n",
    "# Activate it (Windows)\n",
    "venv\\Scripts\\activate\n",
    "\n",
    "# Your prompt should now show (venv)\n",
    "```\n",
    "\n",
    "### Installing Packages\n",
    "\n",
    "```bash\n",
    "# Install packages\n",
    "pip install pandas scikit-learn matplotlib\n",
    "\n",
    "# See what's installed\n",
    "pip list\n",
    "\n",
    "# Save to requirements.txt\n",
    "pip freeze > requirements.txt\n",
    "\n",
    "# Deactivate when done\n",
    "deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Working with requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what packages are installed in this environment\n",
    "import pkg_resources\n",
    "\n",
    "# Get installed packages\n",
    "installed = [(d.project_name, d.version) for d in pkg_resources.working_set]\n",
    "installed.sort()\n",
    "\n",
    "print(\"Some installed packages:\")\n",
    "for name, version in installed[:10]:\n",
    "    print(f\"  {name}=={version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a requirements.txt for our project\n",
    "project_packages = [\n",
    "    'pandas',\n",
    "    'numpy',\n",
    "    'scikit-learn',\n",
    "    'matplotlib'\n",
    "]\n",
    "\n",
    "print(\"Example requirements.txt:\\n\")\n",
    "for pkg in project_packages:\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(pkg).version\n",
    "        print(f\"{pkg}=={version}\")\n",
    "    except:\n",
    "        print(f\"{pkg}  # version unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Configuration Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of hardcoding values in your code...\n",
    "\n",
    "# BAD: Hardcoded values\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "model_path = \"/home/nipun/models/netflix.pkl\"  # Breaks on other machines!\n",
    "\n",
    "print(\"BAD: Hardcoded values are not portable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD: Use a config file\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Example config (normally in a separate file)\n",
    "config_yaml = \"\"\"\n",
    "training:\n",
    "  learning_rate: 0.01\n",
    "  batch_size: 32\n",
    "  epochs: 100\n",
    "  random_seed: 42\n",
    "\n",
    "paths:\n",
    "  data: data/processed/\n",
    "  model: models/netflix.pkl\n",
    "  logs: logs/\n",
    "\n",
    "model:\n",
    "  type: random_forest\n",
    "  n_estimators: 100\n",
    "  max_depth: 10\n",
    "\"\"\"\n",
    "\n",
    "# Parse the config\n",
    "config = yaml.safe_load(config_yaml)\n",
    "\n",
    "print(\"Config loaded:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access config values\n",
    "print(f\"Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"Model type: {config['model']['type']}\")\n",
    "print(f\"Data path: {config['paths']['data']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write config to file\n",
    "with open('config.yaml', 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(\"Config saved to config.yaml\")\n",
    "\n",
    "# Read it back\n",
    "with open('config.yaml', 'r') as f:\n",
    "    loaded_config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Loaded back: learning_rate = {loaded_config['training']['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Project Structure Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a reproducible project structure\n",
    "project_structure = {\n",
    "    'netflix-predictor': {\n",
    "        'data': {\n",
    "            'raw': {},\n",
    "            'processed': {}\n",
    "        },\n",
    "        'models': {},\n",
    "        'notebooks': {},\n",
    "        'src': {},\n",
    "        'tests': {},\n",
    "    }\n",
    "}\n",
    "\n",
    "def print_structure(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print(\"  \" * indent + f\"├── {key}/\")\n",
    "        if isinstance(value, dict):\n",
    "            print_structure(value, indent + 1)\n",
    "\n",
    "print(\"Recommended Project Structure:\")\n",
    "print_structure(project_structure)\n",
    "print(\"  \" * 1 + \"├── requirements.txt\")\n",
    "print(\"  \" * 1 + \"├── config.yaml\")\n",
    "print(\"  \" * 1 + \"├── README.md\")\n",
    "print(\"  \" * 1 + \"└── .gitignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Creating Essential Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample .gitignore\n",
    "gitignore_content = \"\"\"# Data files (too large for Git)\n",
    "data/raw/\n",
    "*.csv\n",
    "*.parquet\n",
    "\n",
    "# Models (too large)\n",
    "models/*.pkl\n",
    "models/*.pth\n",
    "*.h5\n",
    "\n",
    "# Virtual environment\n",
    "venv/\n",
    "env/\n",
    "\n",
    "# Python cache\n",
    "__pycache__/\n",
    "*.pyc\n",
    "*.pyo\n",
    "\n",
    "# Jupyter checkpoints\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "# Secrets (NEVER commit these!)\n",
    ".env\n",
    "secrets.yaml\n",
    "*.key\n",
    "\n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "\n",
    "# Logs\n",
    "*.log\n",
    "logs/\n",
    "\"\"\"\n",
    "\n",
    "print(\".gitignore example:\")\n",
    "print(gitignore_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample README\n",
    "readme_content = \"\"\"# Netflix Movie Predictor\n",
    "\n",
    "Predicts whether a movie will be successful based on features.\n",
    "\n",
    "## Setup\n",
    "\n",
    "1. Create virtual environment:\n",
    "```bash\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # Mac/Linux\n",
    "```\n",
    "\n",
    "2. Install dependencies:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "Train the model:\n",
    "```bash\n",
    "python src/train.py\n",
    "```\n",
    "\n",
    "Make predictions:\n",
    "```bash\n",
    "python src/predict.py --input data/test.csv\n",
    "```\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "- `data/` - Raw and processed data\n",
    "- `models/` - Trained model files\n",
    "- `src/` - Source code\n",
    "- `notebooks/` - Jupyter notebooks for exploration\n",
    "\n",
    "## Authors\n",
    "\n",
    "- Your Name\n",
    "\"\"\"\n",
    "\n",
    "print(\"README.md example:\")\n",
    "print(readme_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Complete Reproducible Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete reproducible training script\n",
    "training_script = '''\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Train the Netflix movie predictor model.\"\"\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set all random seeds.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def load_config(path=\"config.yaml\"):\n",
    "    \"\"\"Load configuration from YAML file.\"\"\"\n",
    "    with open(path) as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def main():\n",
    "    # Load config\n",
    "    config = load_config()\n",
    "    \n",
    "    # Set random seed\n",
    "    set_seed(config[\"training\"][\"random_seed\"])\n",
    "    print(f\"Random seed: {config['training']['random_seed']}\")\n",
    "    \n",
    "    # Load data\n",
    "    data = pd.read_csv(config[\"paths\"][\"data\"] + \"movies.csv\")\n",
    "    print(f\"Loaded {len(data)} samples\")\n",
    "    \n",
    "    # Prepare features\n",
    "    X = data[[\"budget\", \"runtime\", \"star_power\"]]\n",
    "    y = data[\"success\"]\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=config[\"model\"][\"n_estimators\"],\n",
    "        max_depth=config[\"model\"][\"max_depth\"],\n",
    "        random_state=config[\"training\"][\"random_seed\"]\n",
    "    )\n",
    "    \n",
    "    # Cross-validation\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    print(f\"CV Accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "    \n",
    "    # Train final model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Save model\n",
    "    with open(config[\"paths\"][\"model\"], \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved to {config['paths']['model']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "print(\"Complete reproducible training script (train.py):\")\n",
    "print(training_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Reproducibility Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = [\n",
    "    (\"Random seeds set\", True),\n",
    "    (\"Virtual environment created\", True),\n",
    "    (\"requirements.txt with pinned versions\", True),\n",
    "    (\"Config file (no hardcoded values)\", True),\n",
    "    (\"README with setup instructions\", True),\n",
    "    (\".gitignore for data/models\", True),\n",
    "    (\"Proper project structure\", True),\n",
    "    (\"Tested on clean environment\", False),  # You should do this!\n",
    "]\n",
    "\n",
    "print(\"Reproducibility Checklist:\")\n",
    "print(\"=\" * 50)\n",
    "for item, done in checklist:\n",
    "    status = \"✓\" if done else \"○\"\n",
    "    print(f\"  [{status}] {item}\")\n",
    "\n",
    "complete = sum(1 for _, done in checklist if done)\n",
    "print(f\"\\nProgress: {complete}/{len(checklist)} items complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Exercise - Make Your Own Project Reproducible\n",
    "\n",
    "Now it's your turn! For your Netflix predictor project:\n",
    "\n",
    "1. **Create a virtual environment** and install your dependencies\n",
    "2. **Generate requirements.txt** with pinned versions\n",
    "3. **Add random seeds** to all your training scripts\n",
    "4. **Create a config.yaml** for hyperparameters and paths\n",
    "5. **Write a README.md** with setup instructions\n",
    "6. **Create a .gitignore** to exclude data and models\n",
    "7. **Test it!** Clone your repo fresh and follow your own instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import os\n",
    "if os.path.exists('config.yaml'):\n",
    "    os.remove('config.yaml')\n",
    "    print(\"Cleaned up config.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
